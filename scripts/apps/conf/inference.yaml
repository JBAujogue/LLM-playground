- zephyr-7b-gguf-cpu:
    engine: ctransformers
    model_config:
      model_path_or_repo_id: TheBloke/zephyr-7B-beta-GGUF
      model_file: zephyr-7b-beta.Q5_K_M.gguf
      model_type: mistral
    prompt_config:
      template: '''<|system|>\n{system}</s>\n<|user|>\n{user}</s>\n<|assistant|>'''
      fields:
        system: 'Answer the user query, in french, in minimalistic style.'
    generation_config:
      max_new_tokens: 32
      stream: true
    panel_config:
      stream: true


- zephyr-7b-gguf-cpu-2:
    engine: langchain-ctransformers
    model_config:
      model: TheBloke/zephyr-7B-beta-GGUF
      model_file: zephyr-7b-beta.Q5_K_M.gguf
    prompt_config:
      template: '''<|system|>\n{system}</s>\n<|user|>\n{user}</s>\n<|assistant|>'''
      input_variables:
        - system
        - user
      fields:
        system: 'Answer the user query, in french, in minimalistic style.'
    generation_config:
      max_new_tokens: 32
    panel_config:
      stream: false


- mistral-7b-instruct-v0.2-gguf-cpu:
    engine: ctransformers
    model_config:
      model_path_or_repo_id: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
      model_file: mistral-7b-instruct-v0.2.Q6_K.gguf
      model_type: mistral
    prompt_config:
      template: '''<s>[INST] {user} [/INST]'''
      input_variables:
        - user
      fields:
    generation_config:
      max_new_tokens: 1024
      stream: true
    panel_config:
      stream: true


# - stable-code-3b-cpu:
#     engine: huggingface-pipeline
#     model_config:
#       task: text-generation
#       model: stabilityai/stable-code-3b
#       device_map: cpu
#       trust_remote_code: True
#       torch_dtype: auto
#     prompt_config:
#       template: '''<|system|>\n{system}</s>\n<|user|>\n{user}</s>\n<|assistant|>'''
#       input_variables:
#         - system
#         - user
#       fields:
#         system: 'Answer the user query, in french, in minimalistic style.'
#     generation_config:
#       max_new_tokens: 32
#     panel_config:
#       stream: false


# NOT OK
# - zephyr-7b-gptq-cuda:
#   loader: transformers-pipeline
#   device: &device cuda
#   config:
#     kwargs:
#       task: text-generation
#       model: TheBloke/zephyr-7B-beta-GPTQ
#       device_map: *device

